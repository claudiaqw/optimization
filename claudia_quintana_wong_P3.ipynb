{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Genera datos aleatorios, al menos $m = 1000$ observaciones, de un modelo de regresión lineal \n",
    "$y=\\beta_{0}+\\beta_{1}x_1+\\dots+\\beta_{n}x_n+e$ predefinido con al menos $n = 100$ variables. Considera que los coeficientes de la regresión $\\beta_{0}, ...\\beta_{i}, ..., \\beta_{n}$ son enteros donde:\n",
    "$-5 <= \\beta_{i} <= -5$ para todo $i$.\n",
    "\n",
    "Considera también que los errores (residuos) son normales $e\\sim N(0,\\sigma^{2})$ e independientes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000 ## m obeservaciones\n",
    "n = 100\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x_0, x_1 = np.ones([m, 1]), np.random.uniform(0, 10, ([m, n])) #beta0 = 1\n",
    "X = np.concatenate([x_0, x_1],axis=1)\n",
    "\n",
    "beta = np.random.randint(-5, 5, size=([n + 1, 1]))\n",
    "\n",
    "error = np.random.normal(0, 1, (m, 1))#error normal aleatorio\n",
    "\n",
    "Y = np.dot(X,beta) + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,1], Y),plt.xlabel('X'),plt.ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Estimar el valor de los parámetros de la regresión aplicando la solución analítica del problema de mínimos cuadrados.\n",
    "\n",
    "  \\begin{align*}\n",
    "\\text{minimize}\\quad & ||y-X\\beta||_2^2\n",
    "\\end{align*}\n",
    "\n",
    "La solución explícita es: $\\beta_{ls}=(X^T X)^{-1}X^T y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explicit_solution(X, Y):    \n",
    "    return np.dot(np.dot(inv(np.dot((X.T),X)),X.T),Y)\n",
    "\n",
    "explicit_beta = explicit_solution(X, Y)\n",
    "explicit_beta.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Estimar el valor de los coeficientes de la regresión por mínimos cuadrados usando la herramienta minimize del paquete de Python **Scipy.optimize**. Prueba al menos cuatro solvers diferentes y compara su eficiencia en términos de: número de iteraciones totales, número de evaluaciones de la función objetivo, gradiente y hesiano, así como el tiempo de\n",
    "cómputo total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squared(beta, X, Y):\n",
    "    beta = np.matrix(beta)\n",
    "    z = Y - X * (beta.T)\n",
    "    return np.dot(z.T,z)\n",
    "\n",
    "#gradient\n",
    "def derivative(beta, X, Y):\n",
    "    beta = np.matrix(beta)    \n",
    "    pp = -2 * np.dot((Y - np.dot(X,beta.T)).T,X)\n",
    "    aa = np.squeeze(np.asarray(pp))\n",
    "    return aa\n",
    "\n",
    "def hess(beta, X, Y):\n",
    "    return 2 * np.dot(np.transpose(X),X)\n",
    "\n",
    "def linear_regression(beta, X):\n",
    "    print('beta', beta)\n",
    "    beta = np.matrix(beta)\n",
    "    return np.dot(X,beta.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
    "# - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
    "# - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
    "# - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
    "# - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
    "# - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
    "# - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
    "# - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
    "# - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
    "# - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
    "# - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
    "# - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
    "# - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
    "# - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solvers = ['BFGS', 'Newton-CG', 'L-BFGS-B', 'SLSQP']\n",
    "results = {key:{} for key in solvers}\n",
    "\n",
    "beta_ls0 = np.zeros(n+1)\n",
    "for solver in solvers:\n",
    "    output = minimize(fun=least_squared, x0=beta_ls0, args=(X,Y), method=solver, hess=hess, jac=derivative, options={'disp': True})\n",
    "    print(output)\n",
    "    print('---------------------')\n",
    "    results[solver]['success'] = output.success\n",
    "    results[solver]['status'] = output.status\n",
    "    results[solver]['message'] = output.message\n",
    "    results[solver]['nit'] = output.nit    \n",
    "    results[solver]['fun'] = output.fun\n",
    "    results[solver]['x'] = output.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=results)\n",
    "df = df.transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solver: BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(fun=least_squared, x0=beta_ls0, args=(X,Y), method='BFGS', jac=derivative, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Newton-CG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(fun=least_squared, x0=beta_ls0, args=(X,Y), method='Newton-CG', jac=derivative, hess=hess, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L-BFGS-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(fun=least_squared, x0=beta_ls0, args=(X,Y), method='L-BFGS-B', jac=derivative, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SLSQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimize(fun=least_squared, x0=beta_ls0, args=(X,Y), method='SLSQP', jac=derivative, options={'disp': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Estimar el valor de los coeficientes de la regresión por mínimos cuadrados implementando manualmente:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método del Gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\rightarrow$Partimos de un punto inicial $x_0$\n",
    "\n",
    "$\\rightarrow$ Calculamos la dirección de descenso $p_k$\n",
    "\n",
    "$\\rightarrow$ Mientras que estemos lejos de la solución, calculamos la longitud de paso $\\alpha_k>0$\n",
    "\n",
    "$\\rightarrow$ Movemos el punto:\n",
    "$$x_{k+1} = x_k + \\alpha_k\\ p_k$$\n",
    "Hasta la convergencia a una solución local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a,b)=X.shape\n",
    "# beta0 = np.zeros(b) #punto inicial\n",
    "# alpha=0.00005\n",
    "# n_iter=10000 #máximo número de iteraciones\n",
    "# OF_iter=np.zeros(n_iter)\n",
    "# i=0;\n",
    "# tol=1000;\n",
    "# epsilon=1e-3;\n",
    "\n",
    "# time_start = time.time()\n",
    "# while (i <= n_iter - 2) and (tol > epsilon):\n",
    "#    ...\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "def descent(X, alpha=0.00005, epsilon=1e-3, n_iter=100, function_pk=derivative):    \n",
    "    beta_k = np.matrix(np.zeros(X.shape[1]))\n",
    "    k, norm = 0, 1\n",
    "    sigma, beta, s = 0.0005, 0.5, 1\n",
    "    alpha = s\n",
    "    while(norm > epsilon and k < n_iter):\n",
    "        k += 1\n",
    "        grad = function_pk(beta_k, X, Y)        \n",
    "        p_k = -1 * grad\n",
    "        exp = 0\n",
    "        while(linear_regression(beta_k + alpha * p_k, X) > \n",
    "              linear_regression(beta_k, X) + sigma*alpha*np.dot(p_k.T,grad)).all():            \n",
    "            alpha = beta**exp * s\n",
    "            exp += 1\n",
    "            print(exp)\n",
    "            print(alpha)\n",
    "        beta_k = beta_k + alpha * p_k\n",
    "        norm = np.linalg.norm(p_k, ord = 2)        \n",
    "    return beta_k\n",
    "\n",
    "def gradient_descent(X, alpha=0.00005,epsilon=1e-3, n_iter=100):\n",
    "    return descent(X, alpha, epsilon, n_iter, function_pk=derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas = gradient_descent(X)\n",
    "betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método de Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(X, alpha=0.00005,epsilon=1e-3, n_iter=1000):\n",
    "    return descent(X, alpha, epsilon, n_iter, function_pk=hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = newton(X)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método de Quasi-Newton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quasi_newton(X, alpha=0.00005,epsilon=1e-3, n_iter=1000):\n",
    "    beta_ls0 = np.zeros(n+1)\n",
    "    beta_k = hess(beta_ls0, X, Y)\n",
    "    k, norm = 0, 1   \n",
    "    while(norm > epsilon and k < n_iter):\n",
    "        p_k = descent_quasi_newton(beta_k, X, Y)\n",
    "        beta_k = beta_k - alpha * p_k\n",
    "        norm = np.linalg.norm(p_k, ord = 2)\n",
    "        k += 1\n",
    "    return beta_k   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quasi_newton(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_quasi_newton(beta_k, X, Y):\n",
    "    return -1 * np.dot(inv(beta_k), derivative(beta_k, X, Y))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
